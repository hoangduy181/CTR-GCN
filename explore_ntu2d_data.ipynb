{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Explore NTU60 2D Data Structure\n",
        "\n",
        "This notebook explores the `data/ntu2d/ntu60_2d.pkl` file to understand:\n",
        "1. Data format and structure\n",
        "2. Number of joints (checking if it's 17)\n",
        "3. Data shape and dimensions\n",
        "4. Compatibility with CTR-GCN system"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File: data/ntu2d/ntu60_2d.pkl\n",
            "Size: 582.00 MB\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import sys\n",
        "import os\n",
        "\n",
        "filepath = 'data/ntu2d/ntu60_2d.pkl'\n",
        "print(f\"File: {filepath}\")\n",
        "print(f\"Size: {os.path.getsize(filepath) / (1024*1024):.2f} MB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Successfully loaded with standard pickle\n",
            "Type: <class 'dict'>\n",
            "Keys: ['split', 'annotations']\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    with open(filepath, 'rb') as f:\n",
        "        data = pickle.load(f)\n",
        "    print(\"‚úì Successfully loaded with standard pickle\")\n",
        "    print(f\"Type: {type(data)}\")\n",
        "    if isinstance(data, dict):\n",
        "        print(f\"Keys: {list(data.keys())[:10]}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚úó Standard load failed: {type(e).__name__}: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict_keys(['xsub_train', 'xsub_val', 'xview_train', 'xview_val'])"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "split_data = data['split']\n",
        "split_data.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "data split size 4\n",
            "xsub_train size 40091\n",
            "type xsub_train[0] <class 'str'>\n",
            "sample xsub_train[0] S001C001P001R001A001\n",
            "xsub_val size 16487\n",
            "type xsub_val[0] <class 'str'>\n",
            "sample xsub_val[0] S001C001P003R001A001\n",
            "data ratio 0.7085969811587542\n"
          ]
        }
      ],
      "source": [
        "print('data split size', len(split_data))\n",
        "xsub_train = split_data['xsub_train']\n",
        "print('xsub_train size', len(xsub_train))\n",
        "print('type xsub_train[0]', type(xsub_train[0]))\n",
        "print('sample xsub_train[0]', xsub_train[0])\n",
        "xsub_val = split_data['xsub_val']\n",
        "print('xsub_val size', len(xsub_val))\n",
        "print('type xsub_val[0]', type(xsub_val[0]))\n",
        "print('sample xsub_val[0]', xsub_val[0])\n",
        "print('data ratio', len(xsub_train) / (len(xsub_train) + len(xsub_val)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Structure Analysis\n",
        "\n",
        "Based on the exploration, we found:\n",
        "- **17 joints** ‚úÖ (confirmed from keypoint shape)\n",
        "- **2D coordinates** ‚úÖ (shape shows 2 in last dimension)\n",
        "- **Data format**: `(M, T, V, C)` = `(1, T, 17, 2)` where:\n",
        "  - M=1 (persons)\n",
        "  - T=variable frames per sample\n",
        "  - V=17 (joints)\n",
        "  - C=2 (X, Y coordinates)\n",
        "\n",
        "### Key Findings:\n",
        "1. Top-level structure: `{'split': {...}, 'annotations': [...]}`\n",
        "2. Split keys: `xsub_train`, `xsub_val`, `xview_train`, `xview_val`\n",
        "3. Each annotation contains:\n",
        "   - `keypoint`: shape `(1, T, 17, 2)` - skeleton data\n",
        "   - `keypoint_score`: shape `(1, T, 17)` - confidence scores\n",
        "   - `label`: 0-indexed action class\n",
        "   - `total_frames`: number of frames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "DETAILED DATA ANALYSIS\n",
            "============================================================\n",
            "\n",
            "1. Sample Analysis:\n",
            "\n",
            "  Sample 0:\n",
            "    frame_dir: S001C001P001R001A001\n",
            "    label: 0\n",
            "    total_frames: 103\n",
            "    keypoint shape: (1, 103, 17, 2)\n",
            "    keypoint dtype: float16\n",
            "    keypoint_score shape: (1, 103, 17)\n",
            "\n",
            "  Sample 100:\n",
            "    frame_dir: S001C001P001R002A041\n",
            "    label: 40\n",
            "    total_frames: 62\n",
            "    keypoint shape: (1, 62, 17, 2)\n",
            "    keypoint dtype: float16\n",
            "    keypoint_score shape: (1, 62, 17)\n",
            "\n",
            "  Sample 1000:\n",
            "    frame_dir: S001C002P001R001A041\n",
            "    label: 40\n",
            "    total_frames: 109\n",
            "    keypoint shape: (1, 109, 17, 2)\n",
            "    keypoint dtype: float16\n",
            "    keypoint_score shape: (1, 109, 17)\n",
            "\n",
            "2. Label Analysis:\n",
            "    Number of unique labels: 60\n",
            "    Label range: 0 - 59\n",
            "    Expected: 0-59 for NTU60 (60 classes)\n",
            "\n",
            "3. Frame Length Analysis:\n",
            "    Min frames: 32\n",
            "    Max frames: 300\n",
            "    Mean frames: 84.5\n",
            "    Median frames: 76\n",
            "\n",
            "4. Keypoint Value Analysis:\n",
            "    Keypoint min: 313.0\n",
            "    Keypoint max: 1105.0\n",
            "    Keypoint mean: 750.00\n",
            "    Keypoint std: inf\n",
            "    Note: Values appear to be pixel coordinates (X, Y)\n",
            "\n",
            "5. Person Count Analysis:\n",
            "    Person counts in first 100 samples: {1, 2}\n",
            "    All samples have M=1 person (single person per sample)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/duyth/miniconda/envs/jupyter_notebooks/lib/python3.10/site-packages/numpy/_core/_methods.py:171: RuntimeWarning: overflow encountered in reduce\n",
            "  arrmean = umr_sum(arr, axis, dtype, keepdims=True, where=where)\n"
          ]
        }
      ],
      "source": [
        "# Analyze data structure in detail\n",
        "print(\"=\" * 60)\n",
        "print(\"DETAILED DATA ANALYSIS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Check a few samples to understand variations\n",
        "print(\"\\n1. Sample Analysis:\")\n",
        "for i in [0, 100, 1000]:\n",
        "    ann = annotations[i]\n",
        "    print(f\"\\n  Sample {i}:\")\n",
        "    print(f\"    frame_dir: {ann['frame_dir']}\")\n",
        "    print(f\"    label: {ann['label']}\")\n",
        "    print(f\"    total_frames: {ann['total_frames']}\")\n",
        "    print(f\"    keypoint shape: {ann['keypoint'].shape}\")\n",
        "    print(f\"    keypoint dtype: {ann['keypoint'].dtype}\")\n",
        "    print(f\"    keypoint_score shape: {ann['keypoint_score'].shape}\")\n",
        "\n",
        "# Check label distribution\n",
        "print(\"\\n2. Label Analysis:\")\n",
        "labels = [ann['label'] for ann in annotations]\n",
        "unique_labels = sorted(set(labels))\n",
        "print(f\"    Number of unique labels: {len(unique_labels)}\")\n",
        "print(f\"    Label range: {min(labels)} - {max(labels)}\")\n",
        "print(f\"    Expected: 0-59 for NTU60 (60 classes)\")\n",
        "\n",
        "# Check frame length distribution\n",
        "print(\"\\n3. Frame Length Analysis:\")\n",
        "frame_lengths = [ann['total_frames'] for ann in annotations]\n",
        "print(f\"    Min frames: {min(frame_lengths)}\")\n",
        "print(f\"    Max frames: {max(frame_lengths)}\")\n",
        "print(f\"    Mean frames: {sum(frame_lengths)/len(frame_lengths):.1f}\")\n",
        "print(f\"    Median frames: {sorted(frame_lengths)[len(frame_lengths)//2]}\")\n",
        "\n",
        "# Check keypoint value ranges\n",
        "print(\"\\n4. Keypoint Value Analysis:\")\n",
        "sample_keypoints = annotations[0]['keypoint']\n",
        "print(f\"    Keypoint min: {sample_keypoints.min()}\")\n",
        "print(f\"    Keypoint max: {sample_keypoints.max()}\")\n",
        "print(f\"    Keypoint mean: {sample_keypoints.mean():.2f}\")\n",
        "print(f\"    Keypoint std: {sample_keypoints.std():.2f}\")\n",
        "print(f\"    Note: Values appear to be pixel coordinates (X, Y)\")\n",
        "\n",
        "# Check if all samples have same person count\n",
        "print(\"\\n5. Person Count Analysis:\")\n",
        "person_counts = [ann['keypoint'].shape[0] for ann in annotations[:100]]\n",
        "unique_person_counts = set(person_counts)\n",
        "print(f\"    Person counts in first 100 samples: {unique_person_counts}\")\n",
        "print(f\"    All samples have M=1 person (single person per sample)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compatibility with CTR-GCN System\n",
        "\n",
        "### ‚úÖ What Works:\n",
        "1. **17 joints confirmed** - Can create `graph/joint17.py`\n",
        "2. **2D coordinates** - Model can handle with `in_channels=2`\n",
        "3. **Single person per sample** - `num_person=1`\n",
        "4. **Variable frame lengths** - Feeder can handle with `window_size` and `valid_crop_resize`\n",
        "\n",
        "### ‚ö†Ô∏è What Needs Adaptation:\n",
        "\n",
        "1. **Data Format Conversion**:\n",
        "   - Current: `annotations` list with dict format\n",
        "   - CTR-GCN expects: Array format `(N, C, T, V, M)` or feeder that returns `(C, T, V, M)`\n",
        "   - Need to convert: `(1, T, 17, 2)` ‚Üí `(2, T, 17, 1)` for CTR-GCN\n",
        "\n",
        "2. **Feeder Implementation**:\n",
        "   - Create `feeders/feeder_ntu_2d.py`\n",
        "   - Load from pickle file\n",
        "   - Map sample names from `split` to `annotations`\n",
        "   - Convert format: `(M, T, V, C)` ‚Üí `(C, T, V, M)`\n",
        "   - Handle variable frame lengths with `valid_crop_resize`\n",
        "\n",
        "3. **Graph Structure**:\n",
        "   - Create `graph/joint17.py` with 17-joint skeleton structure\n",
        "   - Need to determine bone connectivity for 17 joints\n",
        "\n",
        "4. **Configuration**:\n",
        "   - `num_point: 17`\n",
        "   - `in_channels: 2` (for 2D coordinates)\n",
        "   - `num_person: 1`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "FORMAT CONVERSION TEST\n",
            "============================================================\n",
            "\n",
            "Original shape: (1, 103, 17, 2)\n",
            "  Format: (M=1, T=103, V=17, C=2)\n",
            "\n",
            "After removing M dimension: (103, 17, 2)\n",
            "  Format: (T=103, V=17, C=2)\n",
            "\n",
            "After transpose: (2, 103, 17)\n",
            "  Format: (C=2, T=103, V=17)\n",
            "\n",
            "Final CTR-GCN format: (2, 103, 17, 1)\n",
            "  Format: (C=2, T=103, V=17, M=1)\n",
            "\n",
            "‚úÖ Conversion successful!\n",
            "   CTR-GCN expects: (C, T, V, M) = (2, T, 17, 1)\n"
          ]
        }
      ],
      "source": [
        "# Test data format conversion\n",
        "print(\"=\" * 60)\n",
        "print(\"FORMAT CONVERSION TEST\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Get a sample\n",
        "sample = annotations[0]\n",
        "keypoint = sample['keypoint']  # Shape: (1, T, 17, 2)\n",
        "\n",
        "print(f\"\\nOriginal shape: {keypoint.shape}\")\n",
        "print(f\"  Format: (M=1, T={keypoint.shape[1]}, V=17, C=2)\")\n",
        "\n",
        "# Convert to CTR-GCN format: (C, T, V, M)\n",
        "# Step 1: Remove M dimension (since M=1) -> (T, V, C)\n",
        "keypoint_reshaped = keypoint[0]  # Shape: (T, 17, 2)\n",
        "print(f\"\\nAfter removing M dimension: {keypoint_reshaped.shape}\")\n",
        "print(f\"  Format: (T={keypoint_reshaped.shape[0]}, V=17, C=2)\")\n",
        "\n",
        "# Step 2: Transpose to (C, T, V)\n",
        "keypoint_ctrgcn = keypoint_reshaped.transpose(2, 0, 1)  # Shape: (2, T, 17)\n",
        "print(f\"\\nAfter transpose: {keypoint_ctrgcn.shape}\")\n",
        "print(f\"  Format: (C=2, T={keypoint_ctrgcn.shape[1]}, V=17)\")\n",
        "\n",
        "# Step 3: Add M dimension back: (C, T, V, M)\n",
        "keypoint_final = keypoint_ctrgcn[:, :, :, np.newaxis]  # Shape: (2, T, 17, 1)\n",
        "print(f\"\\nFinal CTR-GCN format: {keypoint_final.shape}\")\n",
        "print(f\"  Format: (C=2, T={keypoint_final.shape[1]}, V=17, M=1)\")\n",
        "\n",
        "print(\"\\n‚úÖ Conversion successful!\")\n",
        "print(\"   CTR-GCN expects: (C, T, V, M) = (2, T, 17, 1)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "LABEL ANALYSIS\n",
            "============================================================\n",
            "\n",
            "Total samples: 56578\n",
            "Unique labels: 60\n",
            "\n",
            "Label distribution (first 10):\n",
            "  Label 0: 940 samples\n",
            "  Label 1: 941 samples\n",
            "  Label 2: 938 samples\n",
            "  Label 3: 940 samples\n",
            "  Label 4: 942 samples\n",
            "  Label 5: 943 samples\n",
            "  Label 6: 944 samples\n",
            "  Label 7: 941 samples\n",
            "  Label 8: 936 samples\n",
            "  Label 9: 941 samples\n",
            "\n",
            "Label range: 0 - 59\n",
            "‚úÖ Labels are 0-indexed and continuous\n",
            "\n",
            "Split consistency check:\n",
            "  Train samples: 40091\n",
            "  Val samples: 16487\n",
            "  Overlap: 0\n",
            "‚úÖ No overlap between train and val splits\n"
          ]
        }
      ],
      "source": [
        "# Check label mapping\n",
        "print(\"=\" * 60)\n",
        "print(\"LABEL ANALYSIS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Count labels\n",
        "from collections import Counter\n",
        "label_counts = Counter(labels)\n",
        "print(f\"\\nTotal samples: {len(labels)}\")\n",
        "print(f\"Unique labels: {len(label_counts)}\")\n",
        "print(f\"\\nLabel distribution (first 10):\")\n",
        "for label in sorted(label_counts.keys())[:10]:\n",
        "    print(f\"  Label {label}: {label_counts[label]} samples\")\n",
        "\n",
        "# Check if labels are 0-indexed and continuous\n",
        "print(f\"\\nLabel range: {min(labels)} - {max(labels)}\")\n",
        "if max(labels) == len(label_counts) - 1:\n",
        "    print(\"‚úÖ Labels are 0-indexed and continuous\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Labels may not be continuous\")\n",
        "\n",
        "# Verify train/val split consistency\n",
        "print(f\"\\nSplit consistency check:\")\n",
        "train_samples = set(split_data['xsub_train'])\n",
        "val_samples = set(split_data['xsub_val'])\n",
        "print(f\"  Train samples: {len(train_samples)}\")\n",
        "print(f\"  Val samples: {len(val_samples)}\")\n",
        "print(f\"  Overlap: {len(train_samples & val_samples)}\")\n",
        "if len(train_samples & val_samples) == 0:\n",
        "    print(\"‚úÖ No overlap between train and val splits\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Overlap detected!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary & Next Steps\n",
        "\n",
        "### ‚úÖ Confirmed:\n",
        "1. **17 joints** - Data has exactly 17 joints per skeleton\n",
        "2. **2D coordinates** - X, Y pixel coordinates (not 3D)\n",
        "3. **Data structure** - Well-organized with split and annotations\n",
        "4. **Format conversion** - Can convert to CTR-GCN format `(C, T, V, M)`\n",
        "\n",
        "### üìã Implementation Checklist:\n",
        "\n",
        "1. **Create Graph File** (`graph/joint17.py`):\n",
        "   - Define 17-joint skeleton structure\n",
        "   - Determine bone connectivity (which joints connect to which)\n",
        "   - Create adjacency matrix\n",
        "\n",
        "2. **Create Feeder** (`feeders/feeder_ntu_2d.py`):\n",
        "   - Load pickle file\n",
        "   - Map split samples to annotations\n",
        "   - Convert format: `(M, T, V, C)` ‚Üí `(C, T, V, M)`\n",
        "   - Handle variable frame lengths\n",
        "   - Return data in format: `(C, T, V, M)` where C=2, V=17, M=1\n",
        "\n",
        "3. **Create Config** (`config/ntu2d/default.yaml`):\n",
        "   - `num_point: 17`\n",
        "   - `num_person: 1`\n",
        "   - `graph: graph.joint17.Graph`\n",
        "   - `feeder: feeders.feeder_ntu_2d.Feeder`\n",
        "   - `in_channels: 2` (for 2D coordinates)\n",
        "\n",
        "4. **Test**:\n",
        "   - Load data through feeder\n",
        "   - Initialize model with 17 joints\n",
        "   - Run forward pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "FINAL SUMMARY\n",
            "============================================================\n",
            "\n",
            "‚úÖ Data successfully loaded and analyzed!\n",
            "\n",
            "Key Statistics:\n",
            "  Total samples: 56578\n",
            "  Train samples (xsub): 40091\n",
            "  Val samples (xsub): 16487\n",
            "  Action classes: 60\n",
            "  Joints per skeleton: 17\n",
            "  Coordinate dimensions: 2D (X, Y)\n",
            "  Persons per sample: 1\n",
            "  Frame length range: 32 - 300\n",
            "\n",
            "‚úÖ Data is compatible with CTR-GCN system!\n",
            "\n",
            "Next steps:\n",
            "  1. Create graph/joint17.py (need 17-joint skeleton structure)\n",
            "  2. Create feeders/feeder_ntu_2d.py (data loader)\n",
            "  3. Create config/ntu2d/default.yaml (configuration)\n",
            "  4. Test end-to-end pipeline\n"
          ]
        }
      ],
      "source": [
        "# Final summary\n",
        "print(\"=\" * 60)\n",
        "print(\"FINAL SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\n‚úÖ Data successfully loaded and analyzed!\")\n",
        "print(f\"\\nKey Statistics:\")\n",
        "print(f\"  Total samples: {len(annotations)}\")\n",
        "print(f\"  Train samples (xsub): {len(split_data['xsub_train'])}\")\n",
        "print(f\"  Val samples (xsub): {len(split_data['xsub_val'])}\")\n",
        "print(f\"  Action classes: {len(unique_labels)}\")\n",
        "print(f\"  Joints per skeleton: 17\")\n",
        "print(f\"  Coordinate dimensions: 2D (X, Y)\")\n",
        "print(f\"  Persons per sample: 1\")\n",
        "print(f\"  Frame length range: {min(frame_lengths)} - {max(frame_lengths)}\")\n",
        "\n",
        "print(f\"\\n‚úÖ Data is compatible with CTR-GCN system!\")\n",
        "print(f\"\\nNext steps:\")\n",
        "print(f\"  1. Create graph/joint17.py (need 17-joint skeleton structure)\")\n",
        "print(f\"  2. Create feeders/feeder_ntu_2d.py (data loader)\")\n",
        "print(f\"  3. Create config/ntu2d/default.yaml (configuration)\")\n",
        "print(f\"  4. Test end-to-end pipeline\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "data total size 56578\n",
            "type annotations[0] <class 'dict'>\n",
            "sample annotations[0] {'frame_dir': 'S001C001P001R001A001', 'label': 0, 'img_shape': (1080, 1920), 'original_shape': (1080, 1920), 'total_frames': 103, 'keypoint': array([[[[1032. ,  334.8],\n",
            "         [1041. ,  325.8],\n",
            "         [1023.5,  325.8],\n",
            "         ...,\n",
            "         [1028. ,  611.5],\n",
            "         [1063. ,  704. ],\n",
            "         [1037. ,  695. ]],\n",
            "\n",
            "        [[1032. ,  334. ],\n",
            "         [1041. ,  325. ],\n",
            "         [1023. ,  325. ],\n",
            "         ...,\n",
            "         [1027. ,  612.5],\n",
            "         [1063. ,  707. ],\n",
            "         [1036. ,  693.5]],\n",
            "\n",
            "        [[1032. ,  334. ],\n",
            "         [1041. ,  325. ],\n",
            "         [1023. ,  325. ],\n",
            "         ...,\n",
            "         [1027. ,  612.5],\n",
            "         [1063. ,  707. ],\n",
            "         [1036. ,  698. ]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[1037. ,  321.8],\n",
            "         [1050. ,  317.5],\n",
            "         [1033. ,  313. ],\n",
            "         ...,\n",
            "         [1028. ,  612. ],\n",
            "         [1064. ,  704. ],\n",
            "         [1037. ,  695.5]],\n",
            "\n",
            "        [[1039. ,  324. ],\n",
            "         [1048. ,  315.2],\n",
            "         [1035. ,  315.2],\n",
            "         ...,\n",
            "         [1030. ,  611. ],\n",
            "         [1066. ,  703.5],\n",
            "         [1039. ,  695. ]],\n",
            "\n",
            "        [[1037. ,  322. ],\n",
            "         [1050. ,  317.5],\n",
            "         [1033. ,  313.2],\n",
            "         ...,\n",
            "         [1028. ,  613.5],\n",
            "         [1064. ,  701.5],\n",
            "         [1037. ,  697. ]]]], shape=(1, 103, 17, 2), dtype=float16), 'keypoint_score': array([[[0.934 , 0.9766, 0.9736, ..., 0.876 , 0.8857, 0.892 ],\n",
            "        [0.9546, 0.993 , 0.989 , ..., 0.877 , 0.9043, 0.9014],\n",
            "        [0.9536, 0.9937, 0.988 , ..., 0.8867, 0.907 , 0.903 ],\n",
            "        ...,\n",
            "        [0.9365, 0.9043, 0.9414, ..., 0.8955, 0.888 , 0.9033],\n",
            "        [0.9585, 0.9385, 0.939 , ..., 0.8984, 0.9126, 0.9146],\n",
            "        [0.9395, 0.904 , 0.9453, ..., 0.898 , 0.8813, 0.886 ]]],\n",
            "      shape=(1, 103, 17), dtype=float16)}\n",
            "annotations[0].keys() dict_keys(['frame_dir', 'label', 'img_shape', 'original_shape', 'total_frames', 'keypoint', 'keypoint_score'])\n",
            "annotations[0][\"label\"] 0\n",
            "annotations[0][\"img_shape\"] (1080, 1920)\n",
            "annotations[0][\"keypoint\"] (1, 103, 17, 2)\n",
            "annotations[0][\"keypoint_score\"] (1, 103, 17)\n"
          ]
        }
      ],
      "source": [
        "annotations = data['annotations']\n",
        "print(\"data total size\", len(annotations))\n",
        "print('type annotations[0]', type(annotations[0]))\n",
        "print('sample annotations[0]', annotations[0])\n",
        "print('annotations[0].keys()', annotations[0].keys())\n",
        "print('annotations[0][\"label\"]', annotations[0][\"label\"])\n",
        "print('annotations[0][\"img_shape\"]', annotations[0][\"img_shape\"])\n",
        "print('annotations[0][\"keypoint\"]', annotations[0][\"keypoint\"].shape)\n",
        "print('annotations[0][\"keypoint_score\"]', annotations[0][\"keypoint_score\"].shape)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "jupyter_notebooks",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
