{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NTU-RGB+D 60 Data Preparation for Google Colab\n",
    "\n",
    "This notebook prepares NTU-RGB+D 60 dataset for training in Google Colab.\n",
    "\n",
    "## Your Setup\n",
    "- **Zip file location**: `drive/MyDrive/\"Colab Notebooks\"/thesis_outline_colabs/CTR-GCN/data`\n",
    "- **Expected zip file**: Contains `nturgbd_skeletons_s001_to_s017.zip` (NTU60)\n",
    "\n",
    "## What This Notebook Does\n",
    "\n",
    "1. Mounts Google Drive\n",
    "2. Locates and extracts the NTU60 zip file\n",
    "3. Runs three processing scripts:\n",
    "   - `get_raw_skes_data.py` - Extract skeleton data from .skeleton files\n",
    "   - `get_raw_denoised_data.py` - Remove bad/noisy skeletons\n",
    "   - `seq_transformation.py` - Transform and create train/test splits\n",
    "4. Creates final `.npz` files: `NTU60_CS.npz` and `NTU60_CV.npz`\n",
    "\n",
    "## Estimated Time\n",
    "- Total processing: ~30-60 minutes\n",
    "- Each step: ~10-20 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Mount Google Drive and Set Up Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "import zipfile\n",
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Set up your specific paths\n",
    "DRIVE_ROOT = '/content/drive/MyDrive'\n",
    "NTU_RAW_DIR = os.path.join(DRIVE_ROOT, 'Colab Notebooks', 'thesis_outline_colabs', 'CTR-GCN', 'data')\n",
    "PROJECT_ROOT = os.path.join(DRIVE_ROOT, 'Colab Notebooks', 'thesis_outline_colabs', 'CTR-GCN')\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(NTU_RAW_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"‚úÖ Google Drive mounted\")\n",
    "print(f\"üìÅ NTU Raw Data Directory: {NTU_RAW_DIR}\")\n",
    "print(f\"üìÅ Project Root: {PROJECT_ROOT}\")\n",
    "\n",
    "# Change to project root\n",
    "if os.path.exists(PROJECT_ROOT):\n",
    "    os.chdir(PROJECT_ROOT)\n",
    "    print(f\"‚úÖ Changed to project root: {os.getcwd()}\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  Project root not found: {PROJECT_ROOT}\")\n",
    "    print(\"   Please ensure CTR-GCN repository is in Google Drive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install numpy scipy scikit-learn pyyaml tqdm\n",
    "\n",
    "# Verify installation\n",
    "import numpy as np\n",
    "import scipy\n",
    "import sklearn\n",
    "print(f\"‚úÖ NumPy: {np.__version__}\")\n",
    "print(f\"‚úÖ SciPy: {scipy.__version__}\")\n",
    "print(f\"‚úÖ scikit-learn: {sklearn.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Locate and Extract Zip File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find zip files in your data directory\n",
    "zip_files = glob.glob(os.path.join(NTU_RAW_DIR, '*.zip'))\n",
    "print(f\"Found {len(zip_files)} zip file(s) in {NTU_RAW_DIR}:\")\n",
    "for zf in zip_files:\n",
    "    print(f\"  - {os.path.basename(zf)}\")\n",
    "\n",
    "# Find NTU60 zip file\n",
    "zip_file = None\n",
    "for zf in zip_files:\n",
    "    filename = os.path.basename(zf).lower()\n",
    "    if 's001_to_s017' in filename or ('nturgbd' in filename and 'skeleton' in filename):\n",
    "        zip_file = zf\n",
    "        break\n",
    "\n",
    "if zip_file:\n",
    "    print(f\"\\n‚úÖ Found NTU60 zip file: {os.path.basename(zip_file)}\")\n",
    "    \n",
    "    # Extract to nturgbd_raw directory\n",
    "    extract_dir = os.path.join(NTU_RAW_DIR, 'nturgbd_raw')\n",
    "    os.makedirs(extract_dir, exist_ok=True)\n",
    "    \n",
    "    print(f\"Extracting to: {extract_dir}...\")\n",
    "    print(\"This may take a few minutes...\")\n",
    "    \n",
    "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_dir)\n",
    "    \n",
    "    print(\"‚úÖ Extraction complete!\")\n",
    "    \n",
    "    # Verify extraction\n",
    "    skeleton_dir = os.path.join(extract_dir, 'nturgb+d_skeletons')\n",
    "    if os.path.exists(skeleton_dir):\n",
    "        skeleton_files = glob.glob(os.path.join(skeleton_dir, '*.skeleton'))\n",
    "        print(f\"‚úÖ Found {len(skeleton_files)} skeleton files\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è  Expected directory 'nturgb+d_skeletons' not found\")\n",
    "        print(\"   Listing extracted directories:\")\n",
    "        for item in os.listdir(extract_dir):\n",
    "            item_path = os.path.join(extract_dir, item)\n",
    "            if os.path.isdir(item_path):\n",
    "                print(f\"     üìÅ {item}/\")\n",
    "            else:\n",
    "                print(f\"     üìÑ {item}\")\n",
    "else:\n",
    "    print(\"\\n‚ùå NTU60 zip file not found!\")\n",
    "    print(f\"   Please ensure the zip file is in: {NTU_RAW_DIR}\")\n",
    "    print(\"   Expected filename contains: 'nturgbd_skeletons_s001_to_s017' or 's001_to_s017'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Verify CTR-GCN Repository Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CTR-GCN repository structure exists\n",
    "required_dirs = [\n",
    "    'data/ntu',\n",
    "    'data/ntu/statistics',\n",
    "]\n",
    "\n",
    "required_files = [\n",
    "    'data/ntu/get_raw_skes_data.py',\n",
    "    'data/ntu/get_raw_denoised_data.py',\n",
    "    'data/ntu/seq_transformation.py',\n",
    "]\n",
    "\n",
    "print(\"Checking repository structure...\")\n",
    "all_good = True\n",
    "\n",
    "for dir_path in required_dirs:\n",
    "    full_path = os.path.join(PROJECT_ROOT, dir_path)\n",
    "    if os.path.exists(full_path):\n",
    "        print(f\"‚úÖ {dir_path}\")\n",
    "    else:\n",
    "        print(f\"‚ùå Missing: {dir_path}\")\n",
    "        os.makedirs(full_path, exist_ok=True)\n",
    "        print(f\"   Created: {dir_path}\")\n",
    "        all_good = False\n",
    "\n",
    "for file_path in required_files:\n",
    "    full_path = os.path.join(PROJECT_ROOT, file_path)\n",
    "    if os.path.exists(full_path):\n",
    "        print(f\"‚úÖ {file_path}\")\n",
    "    else:\n",
    "        print(f\"‚ùå Missing: {file_path}\")\n",
    "        all_good = False\n",
    "\n",
    "if not all_good:\n",
    "    print(\"\\n‚ö†Ô∏è  Some files/directories are missing.\")\n",
    "    print(\"   Please ensure CTR-GCN repository is complete in Google Drive.\")\n",
    "    print(\"   You may need to clone it:\")\n",
    "    print(f\"   !git clone https://github.com/Uason-Chen/CTR-GCN.git {PROJECT_ROOT}\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ All required files and directories found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Update Paths in Processing Scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Navigate to processing directory\n",
    "processing_dir = os.path.join(PROJECT_ROOT, 'data', 'ntu')\n",
    "os.chdir(processing_dir)\n",
    "print(f\"Current directory: {os.getcwd()}\")\n",
    "\n",
    "# Update paths in get_raw_skes_data.py\n",
    "script_path = 'get_raw_skes_data.py'\n",
    "if os.path.exists(script_path):\n",
    "    with open(script_path, 'r') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    # Calculate relative path from data/ntu to raw skeleton directory\n",
    "    raw_skeleton_dir = os.path.join(NTU_RAW_DIR, 'nturgbd_raw', 'nturgb+d_skeletons')\n",
    "    \n",
    "    # Check if directory exists\n",
    "    if os.path.exists(raw_skeleton_dir):\n",
    "        relative_path = os.path.relpath(raw_skeleton_dir, processing_dir) + '/'\n",
    "        \n",
    "        # Update the path in the script\n",
    "        import re\n",
    "        # Find and replace the skes_path line\n",
    "        pattern = r\"skes_path\\s*=\\s*['\\\"]([^'\\\"]+)['\\\"]\"\n",
    "        \n",
    "        if re.search(pattern, content):\n",
    "            content = re.sub(\n",
    "                pattern,\n",
    "                f\"skes_path = '{relative_path}'\",\n",
    "                content\n",
    "            )\n",
    "            \n",
    "            with open(script_path, 'w') as f:\n",
    "                f.write(content)\n",
    "            \n",
    "            print(f\"‚úÖ Updated skes_path in get_raw_skes_data.py\")\n",
    "            print(f\"   New path: {relative_path}\")\n",
    "            print(f\"   Absolute path: {raw_skeleton_dir}\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è  Could not find skes_path definition in script\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  Raw skeleton directory not found: {raw_skeleton_dir}\")\n",
    "        print(\"   Please check Step 3 (extraction)\")\n",
    "else:\n",
    "    print(f\"‚ùå Script not found: {script_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Run Processing Scripts\n",
    "\n",
    "**Note**: Each step may take 10-20 minutes. Be patient!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6.1: Get Raw Skeleton Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"STEP 1/3: Getting raw skeleton data from .skeleton files\")\n",
    "print(\"=\" * 70)\n",
    "print(\"This step reads all .skeleton files and extracts joint positions.\")\n",
    "print(\"Estimated time: 10-20 minutes\\n\")\n",
    "\n",
    "!python get_raw_skes_data.py\n",
    "\n",
    "print(\"\\n‚úÖ Step 1 complete!\")\n",
    "print(\"   Output: raw_data/raw_skes_data.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6.2: Remove Bad Skeletons (Denoising)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"STEP 2/3: Removing bad skeletons (denoising)\")\n",
    "print(\"=\" * 70)\n",
    "print(\"This step filters out noisy or invalid skeleton sequences.\")\n",
    "print(\"Estimated time: 10-15 minutes\\n\")\n",
    "\n",
    "!python get_raw_denoised_data.py\n",
    "\n",
    "print(\"\\n‚úÖ Step 2 complete!\")\n",
    "print(\"   Output: denoised_data/raw_denoised_joints.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6.3: Transform Sequences and Create Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"STEP 3/3: Transforming sequences\")\n",
    "print(\"=\" * 70)\n",
    "print(\"This step:\")\n",
    "print(\"  - Centers skeletons to first frame\")\n",
    "print(\"  - Aligns all sequences to same length\")\n",
    "print(\"  - Splits into train/test sets (CS and CV)\")\n",
    "print(\"  - Creates final .npz files\")\n",
    "print(\"Estimated time: 10-15 minutes\\n\")\n",
    "\n",
    "!python seq_transformation.py\n",
    "\n",
    "print(\"\\n‚úÖ Step 3 complete!\")\n",
    "print(\"   Output: NTU60_CS.npz and NTU60_CV.npz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Verify Processed Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for processed .npz files\n",
    "npz_files = glob.glob('*.npz')\n",
    "print(f\"Found {len(npz_files)} .npz file(s):\\n\")\n",
    "\n",
    "for npz_file in sorted(npz_files):\n",
    "    size_mb = os.path.getsize(npz_file) / (1024 * 1024)\n",
    "    print(f\"üìÅ {npz_file}\")\n",
    "    print(f\"   Size: {size_mb:.2f} MB\")\n",
    "    \n",
    "    # Load and inspect\n",
    "    try:\n",
    "        data = np.load(npz_file, allow_pickle=True)\n",
    "        print(f\"   Contents:\")\n",
    "        for key in sorted(data.keys()):\n",
    "            arr = data[key]\n",
    "            if isinstance(arr, np.ndarray):\n",
    "                print(f\"     - {key}: shape={arr.shape}, dtype={arr.dtype}\")\n",
    "        print()\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è  Error loading file: {e}\\n\")\n",
    "\n",
    "# Expected files\n",
    "expected = ['NTU60_CS.npz', 'NTU60_CV.npz']\n",
    "missing = [f for f in expected if f not in npz_files]\n",
    "\n",
    "if missing:\n",
    "    print(f\"‚ö†Ô∏è  Missing expected files: {missing}\")\n",
    "else:\n",
    "    print(\"‚úÖ All expected files created successfully!\")\n",
    "    print(f\"\\nüìÇ Files are saved at: {processing_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Test Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add project root to path\n",
    "import sys\n",
    "sys.path.insert(0, PROJECT_ROOT)\n",
    "\n",
    "from feeders.feeder_ntu import Feeder\n",
    "\n",
    "# Test train feeder\n",
    "print(\"Testing train feeder...\")\n",
    "train_data_path = os.path.join(processing_dir, 'NTU60_CS.npz')\n",
    "\n",
    "if os.path.exists(train_data_path):\n",
    "    train_feeder = Feeder(\n",
    "        data_path=train_data_path,\n",
    "        split='train',\n",
    "        window_size=64,\n",
    "        p_interval=[0.95],\n",
    "        random_rot=False,\n",
    "        bone=False,\n",
    "        vel=False\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Train feeder created!\")\n",
    "    print(f\"   Dataset size: {len(train_feeder)} samples\")\n",
    "    \n",
    "    # Test loading a sample\n",
    "    data, label, index = train_feeder[0]\n",
    "    print(f\"\\nSample 0:\")\n",
    "    print(f\"   Data shape: {data.shape}\")\n",
    "    print(f\"   Label: {label}\")\n",
    "    print(f\"   Data dtype: {data.dtype}\")\n",
    "    print(f\"   Data range: [{data.min():.2f}, {data.max():.2f}]\")\n",
    "    \n",
    "    # Test test feeder\n",
    "    print(\"\\nTesting test feeder...\")\n",
    "    test_feeder = Feeder(\n",
    "        data_path=train_data_path,\n",
    "        split='test',\n",
    "        window_size=64,\n",
    "        p_interval=[0.95],\n",
    "        random_rot=False,\n",
    "        bone=False,\n",
    "        vel=False\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Test feeder created!\")\n",
    "    print(f\"   Dataset size: {len(test_feeder)} samples\")\n",
    "    \n",
    "    print(\"\\n‚úÖ Data loading test successful!\")\n",
    "    print(\"\\nüéâ Data preparation complete! You can now start training.\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  Processed data file not found: {train_data_path}\")\n",
    "    print(\"   Please check Step 6 (processing scripts)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Summary\n",
    "\n",
    "### ‚úÖ Setup Complete!\n",
    "\n",
    "Your NTU60 dataset is now prepared in Google Drive:\n",
    "\n",
    "**Processed Data Location**: `{PROJECT_ROOT}/data/ntu/`\n",
    "\n",
    "### Files Created:\n",
    "- `NTU60_CS.npz` - Cross-Subject split (~500 MB - 2 GB)\n",
    "- `NTU60_CV.npz` - Cross-View split (~500 MB - 2 GB)\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **Start Training**:\n",
    "   ```python\n",
    "   # In Colab or local environment\n",
    "   python main.py --config config/nturgbd-cross-subject/default.yaml --device 0\n",
    "   ```\n",
    "\n",
    "2. **Verify Config**: Make sure `data_path` in config points to:\n",
    "   - `data/ntu/NTU60_CS.npz` (relative path from project root)\n",
    "\n",
    "3. **Optional**: Delete intermediate files to save space:\n",
    "   - `raw_data/` directory\n",
    "   - `denoised_data/` directory\n",
    "   - Keep only the `.npz` files for training\n",
    "\n",
    "### File Locations:\n",
    "- **Processed data**: `{PROJECT_ROOT}/data/ntu/NTU60_CS.npz`\n",
    "- **Config file**: `{PROJECT_ROOT}/config/nturgbd-cross-subject/default.yaml`\n",
    "\n",
    "### Troubleshooting:\n",
    "\n",
    "If you encounter issues:\n",
    "\n",
    "1. **\"Skeleton file not found\"**: Check that zip file was extracted correctly\n",
    "2. **\"statistics directory not found\"**: Ensure CTR-GCN repository is complete\n",
    "3. **Memory errors**: Use Colab Pro or process locally\n",
    "4. **Path errors**: Verify all paths are correct in the cells above\n",
    "\n",
    "For detailed troubleshooting, see: `Docs/exploration/feeders/ntu_data_preparation_colab.md`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
