{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NTU60 Dataset Setup for Google Drive\n",
    "\n",
    "This notebook helps you download, process, and upload NTU RGB+D 60 dataset to Google Drive for easy access in Colab and other environments.\n",
    "\n",
    "## Overview\n",
    "- **Dataset Size**: ~5-10 GB (raw + processed)\n",
    "- **Processing Time**: ~30-60 minutes depending on system\n",
    "- **Final Output**: Processed `.npz` files ready for training\n",
    "\n",
    "## Steps:\n",
    "1. Mount Google Drive\n",
    "2. Download or upload NTU60 raw data\n",
    "3. Process the raw data into `.npz` format\n",
    "4. Upload processed files to Google Drive\n",
    "5. Verify setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Mount Google Drive\n",
    "\n",
    "First, mount your Google Drive to access it from Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Set up paths\n",
    "DRIVE_ROOT = '/content/drive/MyDrive'\n",
    "PROJECT_DIR = os.path.join(DRIVE_ROOT, 'CTR-GCN')\n",
    "DATA_DIR = os.path.join(PROJECT_DIR, 'data')\n",
    "NTU_RAW_DIR = os.path.join(DATA_DIR, 'nturgbd_raw')\n",
    "NTU_PROCESSED_DIR = os.path.join(DATA_DIR, 'ntu')\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(NTU_RAW_DIR, exist_ok=True)\n",
    "os.makedirs(NTU_PROCESSED_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Google Drive mounted at: {DRIVE_ROOT}\")\n",
    "print(f\"Project directory: {PROJECT_DIR}\")\n",
    "print(f\"Data directory: {DATA_DIR}\")\n",
    "print(f\"NTU raw data directory: {NTU_RAW_DIR}\")\n",
    "print(f\"NTU processed data directory: {NTU_PROCESSED_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Download NTU60 Dataset\n",
    "\n",
    "**Option A: If you already have the dataset downloaded locally**\n",
    "\n",
    "You can upload it manually to Google Drive at the path shown above, or use the upload method below.\n",
    "\n",
    "**Option B: Download from NTU website**\n",
    "\n",
    "1. Request access: https://rose1.ntu.edu.sg/dataset/actionRecognition\n",
    "2. Download: `nturgbd_skeletons_s001_to_s017.zip` (NTU RGB+D 60)\n",
    "3. Upload to Google Drive at: `{NTU_RAW_DIR}/`\n",
    "\n",
    "**Option C: Use wget (if you have direct download link)**\n",
    "\n",
    "If you have a direct download link, you can use wget below. Otherwise, download manually and upload to Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if raw data already exists\n",
    "import glob\n",
    "\n",
    "raw_data_path = os.path.join(NTU_RAW_DIR, 'nturgb+d_skeletons')\n",
    "zip_file = os.path.join(NTU_RAW_DIR, 'nturgbd_skeletons_s001_to_s017.zip')\n",
    "\n",
    "if os.path.exists(raw_data_path):\n",
    "    print(f\"✅ Raw data already exists at: {raw_data_path}\")\n",
    "    skeleton_files = glob.glob(os.path.join(raw_data_path, '**/*.skeleton'), recursive=True)\n",
    "    print(f\"   Found {len(skeleton_files)} skeleton files\")\n",
    "elif os.path.exists(zip_file):\n",
    "    print(f\"✅ Zip file found: {zip_file}\")\n",
    "    print(\"   Extracting...\")\n",
    "    import zipfile\n",
    "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "        zip_ref.extractall(NTU_RAW_DIR)\n",
    "    print(\"   ✅ Extraction complete!\")\n",
    "else:\n",
    "    print(\"❌ Raw data not found.\")\n",
    "    print(f\"   Please upload 'nturgbd_skeletons_s001_to_s017.zip' to: {NTU_RAW_DIR}\")\n",
    "    print(f\"   Or extract it to: {raw_data_path}\")\n",
    "    \n",
    "    # Option: Use wget if you have a direct link\n",
    "    # Uncomment and add your download link:\n",
    "    # !wget \"YOUR_DOWNLOAD_LINK_HERE\" -O {zip_file}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Clone/Setup CTR-GCN Repository\n",
    "\n",
    "If you haven't already, clone the CTR-GCN repository or upload it to Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Navigate to project directory\n",
    "if not os.path.exists(PROJECT_DIR):\n",
    "    print(f\"Project directory not found. Please clone or upload CTR-GCN to: {PROJECT_DIR}\")\n",
    "    print(\"\\nTo clone:\")\n",
    "    print(f\"  !git clone https://github.com/Uason-Chen/CTR-GCN.git {PROJECT_DIR}\")\n",
    "else:\n",
    "    os.chdir(PROJECT_DIR)\n",
    "    print(f\"✅ Project directory found: {PROJECT_DIR}\")\n",
    "    print(f\"   Current directory: {os.getcwd()}\")\n",
    "    \n",
    "    # Check if processing scripts exist\n",
    "    processing_scripts = [\n",
    "        'data/ntu/get_raw_skes_data.py',\n",
    "        'data/ntu/get_raw_denoised_data.py',\n",
    "        'data/ntu/seq_transformation.py'\n",
    "    ]\n",
    "    \n",
    "    missing_scripts = [s for s in processing_scripts if not os.path.exists(s)]\n",
    "    if missing_scripts:\n",
    "        print(f\"\\n⚠️  Missing processing scripts:\")\n",
    "        for script in missing_scripts:\n",
    "            print(f\"   - {script}\")\n",
    "        print(\"\\n   Please ensure all processing scripts are present.\")\n",
    "    else:\n",
    "        print(\"\\n✅ All processing scripts found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Install Dependencies\n",
    "\n",
    "Install required packages for data processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install numpy scipy tqdm pyyaml\n",
    "\n",
    "# Verify installation\n",
    "import numpy as np\n",
    "import scipy\n",
    "print(f\"✅ NumPy version: {np.__version__}\")\n",
    "print(f\"✅ SciPy version: {scipy.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Process Raw Data\n",
    "\n",
    "Process the raw skeleton data into `.npz` format. This creates:\n",
    "- `NTU60_CS.npz` (Cross-Subject split)\n",
    "- `NTU60_CV.npz` (Cross-View split)\n",
    "\n",
    "**Note**: This step may take 30-60 minutes depending on your system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to processing directory\n",
    "processing_dir = os.path.join(PROJECT_DIR, 'data', 'ntu')\n",
    "if not os.path.exists(processing_dir):\n",
    "    print(f\"⚠️  Processing directory not found: {processing_dir}\")\n",
    "    print(\"   Creating directory...\")\n",
    "    os.makedirs(processing_dir, exist_ok=True)\n",
    "\n",
    "os.chdir(processing_dir)\n",
    "print(f\"Current directory: {os.getcwd()}\")\n",
    "\n",
    "# Check if processing scripts exist\n",
    "scripts = [\n",
    "    'get_raw_skes_data.py',\n",
    "    'get_raw_denoised_data.py', \n",
    "    'seq_transformation.py'\n",
    "]\n",
    "\n",
    "for script in scripts:\n",
    "    if not os.path.exists(script):\n",
    "        print(f\"⚠️  Script not found: {script}\")\n",
    "        print(\"   Please ensure all processing scripts are in data/ntu/ directory\")\n",
    "    else:\n",
    "        print(f\"✅ Found: {script}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5.1: Get raw skeleton data\n",
    "print(\"=\" * 60)\n",
    "print(\"Step 1/3: Getting raw skeleton data...\")\n",
    "print(\"=\" * 60)\n",
    "!python get_raw_skes_data.py\n",
    "print(\"\\n✅ Step 1 complete!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5.2: Remove bad skeletons\n",
    "print(\"=\" * 60)\n",
    "print(\"Step 2/3: Removing bad skeletons...\")\n",
    "print(\"=\" * 60)\n",
    "!python get_raw_denoised_data.py\n",
    "print(\"\\n✅ Step 2 complete!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5.3: Transform sequences\n",
    "print(\"=\" * 60)\n",
    "print(\"Step 3/3: Transforming sequences...\")\n",
    "print(\"=\" * 60)\n",
    "!python seq_transformation.py\n",
    "print(\"\\n✅ Step 3 complete!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Verify Processed Files\n",
    "\n",
    "Check that the processed `.npz` files were created successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for processed files\n",
    "import glob\n",
    "\n",
    "processed_files = glob.glob('*.npz')\n",
    "print(f\"Found {len(processed_files)} processed files:\")\n",
    "for f in processed_files:\n",
    "    size_mb = os.path.getsize(f) / (1024 * 1024)\n",
    "    print(f\"  ✅ {f} ({size_mb:.2f} MB)\")\n",
    "\n",
    "# Expected files\n",
    "expected_files = ['NTU60_CS.npz', 'NTU60_CV.npz']\n",
    "missing_files = [f for f in expected_files if f not in processed_files]\n",
    "\n",
    "if missing_files:\n",
    "    print(f\"\\n⚠️  Missing expected files: {missing_files}\")\n",
    "else:\n",
    "    print(\"\\n✅ All expected files created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Verify Data Structure\n",
    "\n",
    "Load and inspect one of the processed files to ensure it's correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and inspect processed data\n",
    "if 'NTU60_CS.npz' in processed_files:\n",
    "    data = np.load('NTU60_CS.npz', allow_pickle=True)\n",
    "    print(\"NTU60_CS.npz contents:\")\n",
    "    for key in data.keys():\n",
    "        arr = data[key]\n",
    "        if isinstance(arr, np.ndarray):\n",
    "            print(f\"  {key}: shape={arr.shape}, dtype={arr.dtype}\")\n",
    "        else:\n",
    "            print(f\"  {key}: {type(arr)}\")\n",
    "    print(\"\\n✅ Data structure looks good!\")\n",
    "else:\n",
    "    print(\"⚠️  NTU60_CS.npz not found for inspection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Files are Already in Google Drive!\n",
    "\n",
    "Since we're working directly in Google Drive, the processed files are already saved there. \n",
    "\n",
    "**Location**: `{PROJECT_DIR}/data/ntu/`\n",
    "\n",
    "You can now use these files in Colab or any other environment by mounting Google Drive and pointing to this path."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Update Config Files (Optional)\n",
    "\n",
    "If you want to use absolute paths in your config files, you can update them here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Update config file paths\n",
    "config_path = os.path.join(PROJECT_DIR, 'config', 'nturgbd-cross-subject', 'default.yaml')\n",
    "\n",
    "if os.path.exists(config_path):\n",
    "    print(f\"Config file found: {config_path}\")\n",
    "    print(\"\\nCurrent data_path in config:\")\n",
    "    import yaml\n",
    "    with open(config_path, 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "        print(f\"  train_feeder_args.data_path: {config.get('train_feeder_args', {}).get('data_path', 'N/A')}\")\n",
    "        print(f\"  test_feeder_args.data_path: {config.get('test_feeder_args', {}).get('data_path', 'N/A')}\")\n",
    "    \n",
    "    # If you want to use absolute paths, uncomment below:\n",
    "    # config['train_feeder_args']['data_path'] = os.path.join(PROJECT_DIR, 'data', 'ntu', 'NTU60_CS.npz')\n",
    "    # config['test_feeder_args']['data_path'] = os.path.join(PROJECT_DIR, 'data', 'ntu', 'NTU60_CS.npz')\n",
    "    # with open(config_path, 'w') as f:\n",
    "    #     yaml.dump(config, f)\n",
    "    # print(\"\\n✅ Config updated with absolute paths!\")\n",
    "else:\n",
    "    print(f\"Config file not found: {config_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Summary\n",
    "\n",
    "### ✅ Setup Complete!\n",
    "\n",
    "Your NTU60 dataset is now set up in Google Drive:\n",
    "\n",
    "**Raw Data Location**: `{DRIVE_ROOT}/CTR-GCN/data/nturgbd_raw/`  \n",
    "**Processed Data Location**: `{DRIVE_ROOT}/CTR-GCN/data/ntu/`\n",
    "\n",
    "### Files Created:\n",
    "- `NTU60_CS.npz` - Cross-Subject split (for training/testing)\n",
    "- `NTU60_CV.npz` - Cross-View split (for training/testing)\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **In Colab**: Mount Google Drive and use the data directly:\n",
    "   ```python\n",
    "   from google.colab import drive\n",
    "   drive.mount('/content/drive')\n",
    "   # Data is at: /content/drive/MyDrive/CTR-GCN/data/ntu/\n",
    "   ```\n",
    "\n",
    "2. **Training**: Use the config files with relative paths (they should work if you run from project root):\n",
    "   ```bash\n",
    "   python main.py --config config/nturgbd-cross-subject/default.yaml --device 0\n",
    "   ```\n",
    "\n",
    "3. **Storage**: The processed `.npz` files are much smaller than raw data and can be easily shared or backed up.\n",
    "\n",
    "### File Sizes (Approximate):\n",
    "- Raw data: ~5-7 GB\n",
    "- Processed `.npz` files: ~500 MB - 2 GB each\n",
    "- **Total in Google Drive**: ~2-4 GB (just processed files) or ~7-10 GB (with raw data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
